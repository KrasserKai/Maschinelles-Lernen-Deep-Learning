{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naiver Bayesklassifikator zur Gesichtserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie den Gaussian-Naïve-Bayes-Klassifikator aus der Vorlesung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        self.class_prior = {}\n",
    "\n",
    "        # Calculate mean and standard deviation for each feature in each class\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = np.mean(X_c, axis=0)\n",
    "            self.std[c] = np.std(X_c, axis=0)\n",
    "\n",
    "            # Calculate class prior probability\n",
    "            self.class_prior[c] = len(X_c) / len(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        # For every feature\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "\n",
    "            # Calculate posterior probability for each class\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.class_prior[c])\n",
    "                likelihood = np.sum(np.log(norm.pdf(x, loc=self.mean[c], scale=self.std[c])))\n",
    "                posterior = prior + likelihood\n",
    "                posteriors.append(posterior)\n",
    "\n",
    "            # Append the class with the highest posterior probability to predictions\n",
    "            predictions.append(self.classes[np.argmax(posteriors)])\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen Sie Ihre Implementierung am Datensatz ''Labeled Faces in the Wild'' aus Aufgabe 2, wiederum nur für Personen, für die mindestens 70 Bilder existieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilen Sie Ihren Datensatz in 60 % Trainings- und 40% Testdaten (nach vorheriger Zufalls-Permutation der Reihenfolge) und skalieren Sie die Bilder wieder auf 1/8 der Originalgröße."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "data = []\n",
    "names = []\n",
    "\n",
    "# Daten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "    all_person_data = []\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Skalieren auf 32x32\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "            data.append(img_vector)\n",
    "            names.append(person_folder)\n",
    "\n",
    "# Kombiniere die beiden Listen zu einer Liste von Tupeln\n",
    "data_and_names = list(zip(data, names))\n",
    "\n",
    "# Zufalls-Permutation der Reihenfolge\n",
    "import random\n",
    "random.shuffle(data_and_names)\n",
    "\n",
    "# Teile die kombinierten und zufällig angeordneten Listen in Trainings- und Testlisten auf (60% Train, 40% Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tuples, test_tuples = train_test_split(data_and_names, test_size=0.4, random_state=42)\n",
    "\n",
    "# Teile die Trainings- und Testlisten wieder in die ursprünglichen Listen auf\n",
    "train_data, train_names = zip(*train_tuples)\n",
    "train_data = np.array(train_data)\n",
    "test_data, test_names = zip(*test_tuples)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Führen Sie anschließend eine Hauptkomponentenanalyse auf den Trainingsdaten durch und projizieren Sie sowohl Trainings- als auch Testbilder auf die ersten 7 Eigengesichter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(train_data)\n",
    "\n",
    "from pca import pca\n",
    "num_components = len(X.columns)\n",
    "X_pca, Sigma, V = pca(X, num_components)\n",
    "\n",
    "# Umrechnung von Singulärwerten in Eigenwerte\n",
    "eigenvalues = (Sigma**2) / (len(X) - 1)\n",
    "\n",
    "# Gesamtvarianz\n",
    "total_variance = sum(eigenvalues)\n",
    "\n",
    "# Erklärte Varianz für jede Komponente\n",
    "explained_variances = [(i / total_variance) * 100 for i in eigenvalues]\n",
    "\n",
    "# Kumulative erklärte Varianz\n",
    "cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Tabellarische Darstellung\n",
    "results = pd.DataFrame({\n",
    "    'Eigenwert': eigenvalues,\n",
    "    'Erklärte Varianz (%)': explained_variances,\n",
    "    'Kumulative erklärte Varianz (%)': cumulative_variances\n",
    "})\n",
    "\n",
    "print(\"Ergebnisse der PCA-Analyse:\")\n",
    "print(results.head(150))\n",
    "\n",
    "# Trainingsdaten projeziert auf die ersten 7 Eigengesichter\n",
    "train_data_pca = X_pca[:,:7]\n",
    "\n",
    "# Testdaten zentrieren\n",
    "test_data_meaned = pd.DataFrame(test_data) - np.mean(X, axis=0)\n",
    "\n",
    "# Testdaten standardisieren\n",
    "test_data_std = np.std(test_data_meaned, axis=0)\n",
    "test_data_standardized = test_data_meaned / test_data_std\n",
    "\n",
    "# Projektion der Testdaten auf die ersten 7 Eigengesichter\n",
    "test_data_pca = np.dot(test_data_standardized, V[:,:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trainieren Sie Ihren GNB-Klassifikator auf dem Trainingsdatensatz als ''George-W.-Bush-Detektor'', d.h. alle zu dieser Person gehörigen Bilder werden mit 1 gelabelt, alle sonstigen mit –1. Werten Sie Ihren Klassifikator sowohl auf den Trainings- wie auf den unabhängigen Testdaten aus. Bestimmen Sie dafür jeweils die Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([1 if name == 'George_W_Bush' else -1 for name in train_names])\n",
    "test_labels = np.array([1 if name == 'George_W_Bush' else -1 for name in test_names])\n",
    "\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(train_data, train_labels)\n",
    "\n",
    "# Vorhersagen für Trainingsdaten\n",
    "train_pred = model.predict(train_data)\n",
    "\n",
    "# True Positives, True Negatives, False Positives und False Negatives\n",
    "tp = np.sum((train_labels == 1) & (train_pred == 1))\n",
    "tn = np.sum((train_labels == -1) & (train_pred == -1))\n",
    "fp = np.sum((train_labels == -1) & (train_pred == 1))\n",
    "fn = np.sum((train_labels == 1) & (train_pred == -1))\n",
    "\n",
    "# Berechnung der Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate\n",
    "detection_probability = tp / (tp + fn)\n",
    "true_negative_rate = tn / (tn + fp)\n",
    "false_alarm_rate = 1 - true_negative_rate\n",
    "false_negative_rate = 1 - detection_probability\n",
    "\n",
    "print(\"---Trainingsdaten---\")\n",
    "print(f\"Detektionswahrscheinlichkeit: {detection_probability}\")\n",
    "print(f\"Richtig-Negativ-Rate: {true_negative_rate}\")\n",
    "print(f\"Fehlalarmrate: {false_alarm_rate}\")\n",
    "print(f\"Falsch-Negativ-Rate: {false_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen für Testdaten\n",
    "test_pred = model.predict(test_data)\n",
    "\n",
    "# True Positives, True Negatives, False Positives und False Negatives\n",
    "tp = np.sum((test_labels == 1) & (test_pred == 1))\n",
    "tn = np.sum((test_labels == -1) & (test_pred == -1))\n",
    "fp = np.sum((test_labels == -1) & (test_pred == 1))\n",
    "fn = np.sum((test_labels == 1) & (test_pred == -1))\n",
    "\n",
    "# Berechnung der Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate\n",
    "detection_probability = tp / (tp + fn)\n",
    "true_negative_rate = tn / (tn + fp)\n",
    "false_alarm_rate = 1 - true_negative_rate\n",
    "false_negative_rate = 1 - detection_probability\n",
    "\n",
    "print(\"---Testdaten---\")\n",
    "print(f\"Detektionswahrscheinlichkeit: {detection_probability}\")\n",
    "print(f\"Richtig-Negativ-Rate: {true_negative_rate}\")\n",
    "print(f\"Fehlalarmrate: {false_alarm_rate}\")\n",
    "print(f\"Falsch-Negativ-Rate: {false_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
