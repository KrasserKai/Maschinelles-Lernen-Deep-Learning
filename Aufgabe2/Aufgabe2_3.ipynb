{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naiver Bayesklassifikator zur Gesichtserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version_information extension is already loaded. To reload it, use:\n",
      "  %reload_ext version_information\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie den Gaussian-Naïve-Bayes-Klassifikator aus der Vorlesung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        self.class_prior = {}\n",
    "\n",
    "        # Calculate mean and standard deviation for each feature in each class\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = np.mean(X_c, axis=0)\n",
    "            self.std[c] = np.std(X_c, axis=0)\n",
    "\n",
    "            # Calculate class prior probability\n",
    "            self.class_prior[c] = len(X_c) / len(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        # For every feature\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "\n",
    "            # Calculate posterior probability for each class\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.class_prior[c])\n",
    "                likelihood = np.sum(np.log(norm.pdf(x, loc=self.mean[c], scale=self.std[c])))\n",
    "                posterior = prior + likelihood\n",
    "                posteriors.append(posterior)\n",
    "\n",
    "            # Append the class with the highest posterior probability to predictions\n",
    "            predictions.append(self.classes[np.argmax(posteriors)])\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen Sie Ihre Implementierung am Datensatz ''Labeled Faces in the Wild'' aus Aufgabe 2, wiederum nur für Personen, für die mindestens 70 Bilder existieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personen mit mindestens 70 Bildern:\n",
      "Ariel_Sharon: 77 Bilder\n",
      "Colin_Powell: 236 Bilder\n",
      "Donald_Rumsfeld: 121 Bilder\n",
      "George_W_Bush: 530 Bilder\n",
      "Gerhard_Schroeder: 109 Bilder\n",
      "Hugo_Chavez: 71 Bilder\n",
      "Tony_Blair: 144 Bilder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilen Sie Ihren Datensatz in 60 % Trainings- und 40% Testdaten (nach vorheriger Zufalls-Permutation der Reihenfolge) und skalieren Sie die Bilder wieder auf 1/8 der Originalgröße."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "data = []\n",
    "names = []\n",
    "\n",
    "# Daten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "    all_person_data = []\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Augen und Mund ausschneiden\n",
    "            img = img[90:185,80:170]\n",
    "            # Skalieren auf 32x32\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "            data.append(img_vector)\n",
    "            names.append(person_folder)\n",
    "\n",
    "# Kombiniere die beiden Listen zu einer Liste von Tupeln\n",
    "data_and_names = list(zip(data, names))\n",
    "\n",
    "# Zufalls-Permutation der Reihenfolge\n",
    "import random\n",
    "random.shuffle(data_and_names)\n",
    "\n",
    "# Teile die kombinierten und zufällig angeordneten Listen in Trainings- und Testlisten auf (60% Train, 40% Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tuples, test_tuples = train_test_split(data_and_names, test_size=0.4, random_state=42)\n",
    "\n",
    "# Teile die Trainings- und Testlisten wieder in die ursprünglichen Listen auf\n",
    "train_data, train_names = zip(*train_tuples)\n",
    "train_data = np.array(train_data)\n",
    "test_data, test_names = zip(*test_tuples)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Führen Sie anschließend eine Hauptkomponentenanalyse auf den Trainingsdaten durch und projizieren Sie sowohl Trainings- als auch Testbilder auf die ersten 7 Eigengesichter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse der PCA-Analyse:\n",
      "      Eigenwert  Erklärte Varianz (%)  Kumulative erklärte Varianz (%)\n",
      "0    237.266747             23.140567                        23.140567\n",
      "1     73.349449              7.153754                        30.294321\n",
      "2     59.431472              5.796337                        36.090657\n",
      "3     55.252134              5.388727                        41.479384\n",
      "4     49.541637              4.831784                        46.311168\n",
      "..          ...                   ...                              ...\n",
      "145    0.424317              0.041384                        96.125193\n",
      "146    0.420139              0.040976                        96.166169\n",
      "147    0.415878              0.040560                        96.206729\n",
      "148    0.409197              0.039909                        96.246638\n",
      "149    0.402574              0.039263                        96.285901\n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_data)\n",
    "\n",
    "from pca import pca\n",
    "num_components = len(X.columns)\n",
    "X_pca, Sigma, V = pca(X, num_components)\n",
    "\n",
    "# Umrechnung von Singulärwerten in Eigenwerte\n",
    "eigenvalues = (Sigma**2) / (len(X) - 1)\n",
    "\n",
    "# Gesamtvarianz\n",
    "total_variance = sum(eigenvalues)\n",
    "\n",
    "# Erklärte Varianz für jede Komponente\n",
    "explained_variances = [(i / total_variance) * 100 for i in eigenvalues]\n",
    "\n",
    "# Kumulative erklärte Varianz\n",
    "cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Tabellarische Darstellung\n",
    "results = pd.DataFrame({\n",
    "    'Eigenwert': eigenvalues,\n",
    "    'Erklärte Varianz (%)': explained_variances,\n",
    "    'Kumulative erklärte Varianz (%)': cumulative_variances\n",
    "})\n",
    "\n",
    "print(\"Ergebnisse der PCA-Analyse:\")\n",
    "print(results.head(150))\n",
    "\n",
    "# Trainingsdaten projeziert auf die ersten 7 Eigengesichter\n",
    "train_data_pca = X_pca[:,:7]\n",
    "\n",
    "# Testdaten zentrieren\n",
    "test_data_meaned = pd.DataFrame(test_data) - np.mean(X, axis=0)\n",
    "\n",
    "# Testdaten standardisieren\n",
    "test_data_std = np.std(test_data_meaned, axis=0)\n",
    "test_data_standardized = test_data_meaned / test_data_std\n",
    "\n",
    "# Projektion der Testdaten auf die ersten 7 Eigengesichter\n",
    "test_data_pca = np.dot(test_data_standardized, V[:,:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trainieren Sie Ihren GNB-Klassifikator auf dem Trainingsdatensatz als ''George-W.-Bush-Detektor'', d.h. alle zu dieser Person gehörigen Bilder werden mit 1 gelabelt, alle sonstigen mit –1. Werten Sie Ihren Klassifikator sowohl auf den Trainings- wie auf den unabhängigen Testdaten aus. Bestimmen Sie dafür jeweils die Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Trainingsdaten---\n",
      "Detektionswahrscheinlichkeit: 0.7049689440993789\n",
      "Richtig-Negativ-Rate: 0.7933333333333333\n",
      "Fehlalarmrate: 0.20666666666666667\n",
      "Falsch-Negativ-Rate: 0.29503105590062106\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.array([1 if name == 'George_W_Bush' else -1 for name in train_names])\n",
    "test_labels = np.array([1 if name == 'George_W_Bush' else -1 for name in test_names])\n",
    "\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(train_data, train_labels)\n",
    "\n",
    "# Vorhersagen für Trainingsdaten\n",
    "train_pred = model.predict(train_data)\n",
    "\n",
    "# True Positives, True Negatives, False Positives und False Negatives\n",
    "tp = np.sum((train_labels == 1) & (train_pred == 1))\n",
    "tn = np.sum((train_labels == -1) & (train_pred == -1))\n",
    "fp = np.sum((train_labels == -1) & (train_pred == 1))\n",
    "fn = np.sum((train_labels == 1) & (train_pred == -1))\n",
    "\n",
    "# Berechnung der Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate\n",
    "detection_probability = tp / (tp + fn)\n",
    "true_negative_rate = tn / (tn + fp)\n",
    "false_alarm_rate = 1 - true_negative_rate\n",
    "false_negative_rate = 1 - detection_probability\n",
    "\n",
    "print(\"---Trainingsdaten---\")\n",
    "print(f\"Detektionswahrscheinlichkeit: {detection_probability}\")\n",
    "print(f\"Richtig-Negativ-Rate: {true_negative_rate}\")\n",
    "print(f\"Fehlalarmrate: {false_alarm_rate}\")\n",
    "print(f\"Falsch-Negativ-Rate: {false_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Testdaten---\n",
      "Detektionswahrscheinlichkeit: 0.6346153846153846\n",
      "Richtig-Negativ-Rate: 0.75\n",
      "Fehlalarmrate: 0.25\n",
      "Falsch-Negativ-Rate: 0.3653846153846154\n"
     ]
    }
   ],
   "source": [
    "# Vorhersagen für Testdaten\n",
    "test_pred = model.predict(test_data)\n",
    "\n",
    "# True Positives, True Negatives, False Positives und False Negatives\n",
    "tp = np.sum((test_labels == 1) & (test_pred == 1))\n",
    "tn = np.sum((test_labels == -1) & (test_pred == -1))\n",
    "fp = np.sum((test_labels == -1) & (test_pred == 1))\n",
    "fn = np.sum((test_labels == 1) & (test_pred == -1))\n",
    "\n",
    "# Berechnung der Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate\n",
    "detection_probability = tp / (tp + fn)\n",
    "true_negative_rate = tn / (tn + fp)\n",
    "false_alarm_rate = 1 - true_negative_rate\n",
    "false_negative_rate = 1 - detection_probability\n",
    "\n",
    "print(\"---Testdaten---\")\n",
    "print(f\"Detektionswahrscheinlichkeit: {detection_probability}\")\n",
    "print(f\"Richtig-Negativ-Rate: {true_negative_rate}\")\n",
    "print(f\"Fehlalarmrate: {false_alarm_rate}\")\n",
    "print(f\"Falsch-Negativ-Rate: {false_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.12.0 64bit [MSC v.1935 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "8.16.1"
        },
        {
         "module": "OS",
         "version": "Windows 11 10.0.22621 SP0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.12.0 64bit [MSC v.1935 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>8.16.1</td></tr><tr><td>OS</td><td>Windows 11 10.0.22621 SP0</td></tr><tr><td colspan='2'>Tue Nov 14 15:53:00 2023 Mitteleuropäische Zeit</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.12.0 64bit [MSC v.1935 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 8.16.1 \\\\ \\hline\n",
       "OS & Windows 11 10.0.22621 SP0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Tue Nov 14 15:53:00 2023 Mitteleuropäische Zeit} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.12.0 64bit [MSC v.1935 64 bit (AMD64)]\n",
       "IPython 8.16.1\n",
       "OS Windows 11 10.0.22621 SP0\n",
       "Tue Nov 14 15:53:00 2023 Mitteleuropäische Zeit"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
