{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naiver Bayesklassifikator zur Gesichtserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie den Gaussian-Naïve-Bayes-Klassifikator aus der Vorlesung. Testen Sie Ihre Implementierung am Datensatz ''Labeled Faces in the Wild'' aus Aufgabe 2, wiederum nur für Personen, für die mindestens 70 Bilder existieren. Teilen Sie Ihren Datensatz in 60 % Trainings- und 40% Testdaten (nach vorheriger Zufalls-Permutation der Reihenfolge) und skalieren Sie die Bilder wieder auf 1/8 der Originalgröße. Führen Sie anschließend eine Hauptkomponentenanalyse auf den Trainingsdaten durch und projizieren Sie sowohl Trainings- als auch Testbilder auf die ersten 7 Eigengesichter. Trainieren Sie Ihren GNB-Klassifikator auf dem Trainingsdatensatz als ''George-W.-Bush-Detektor'', d.h. alle zu dieser Person gehörigen Bilder werden mit 1 gelabelt, alle sonstigen mit –1. Werten Sie Ihren Klassifikator sowohl auf den Trainings- wie auf den unabhängigen Testdaten aus. Bestimmen Sie dafür jeweils die Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def GaussianNaiveBayes(X, y):\n",
    "    classes = np.unique(y)\n",
    "    mean = {}\n",
    "    std = {}\n",
    "    class_prior = {}\n",
    "\n",
    "    # Calculate mean and standard deviation for each feature in each class\n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        mean[c] = np.mean(X_c, axis=0)\n",
    "        std[c] = np.std(X_c, axis=0)\n",
    "\n",
    "        # Calculate class prior probability\n",
    "        class_prior[c] = len(X_c) / len(X)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # For every feature\n",
    "    for x in X:\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate posterior probability for each class\n",
    "        for c in classes:\n",
    "            prior = np.log(class_prior[c])\n",
    "            likelihood = np.sum(np.log(norm.pdf(x, loc=mean[c], scale=std[c])))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Append the class with the highest posterior probability to predictions\n",
    "        predictions.append(classes[np.argmax(posteriors)])\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "dataset_path    = extracted_path\n",
    "train_data_path = \".lfw-dataset-train_2\"\n",
    "test_data_path  = \".lfw-dataset-test_2\"\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "os.makedirs(test_data_path, exist_ok=True)\n",
    "\n",
    "# Daten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(dataset_path, person_folder)\n",
    "    all_person_data = []\n",
    "    train_person_data = []\n",
    "    test_person_data = []\n",
    "    first = True\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Skalieren auf 32x32\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "            all_person_data.append(img_vector)\n",
    "\n",
    "    # Trainingsdaten in einer gemeinsamen Designmatrix speichern\n",
    "    train_person_data = np.array(train_person_data)\n",
    "    np.save(os.path.join(train_data_path, f\"{person_folder}_train.npy\"), train_person_data)\n",
    "\n",
    "    # Testdaten getrennt abspeichern\n",
    "    test_person_data = np.array(test_person_data)\n",
    "    np.save(os.path.join(test_data_path, f\"{person_folder}_test.npy\"), test_person_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
