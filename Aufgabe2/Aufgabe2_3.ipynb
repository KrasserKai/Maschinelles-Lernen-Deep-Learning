{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naiver Bayesklassifikator zur Gesichtserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie den Gaussian-Naïve-Bayes-Klassifikator aus der Vorlesung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def GaussianNaiveBayes(X, y):\n",
    "    classes = np.unique(y)\n",
    "    mean = {}\n",
    "    std = {}\n",
    "    class_prior = {}\n",
    "\n",
    "    # Calculate mean and standard deviation for each feature in each class\n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        mean[c] = np.mean(X_c, axis=0)\n",
    "        std[c] = np.std(X_c, axis=0)\n",
    "\n",
    "        # Calculate class prior probability\n",
    "        class_prior[c] = len(X_c) / len(X)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # For every feature\n",
    "    for x in X:\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate posterior probability for each class\n",
    "        for c in classes:\n",
    "            prior = np.log(class_prior[c])\n",
    "            likelihood = np.sum(np.log(norm.pdf(x, loc=mean[c], scale=std[c])))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Append the class with the highest posterior probability to predictions\n",
    "        predictions.append(classes[np.argmax(posteriors)])\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen Sie Ihre Implementierung am Datensatz ''Labeled Faces in the Wild'' aus Aufgabe 2, wiederum nur für Personen, für die mindestens 70 Bilder existieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilen Sie Ihren Datensatz in 60 % Trainings- und 40% Testdaten (nach vorheriger Zufalls-Permutation der Reihenfolge) und skalieren Sie die Bilder wieder auf 1/8 der Originalgröße."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "data = []\n",
    "names = []\n",
    "\n",
    "# Daten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "    all_person_data = []\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Skalieren auf 32x32\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "            data.append(img_vector)\n",
    "            names.append(person_folder)\n",
    "\n",
    "    # Daten in einer gemeinsamen Designmatrix speichern\n",
    "    #all_person_data = np.array(all_person_data)\n",
    "    #data.append(all_person_data)\n",
    "\n",
    "# Kombiniere die beiden Listen zu einer Liste von Tupeln\n",
    "data_and_names = list(zip(data, names))\n",
    "\n",
    "# Zufalls-Permutation der Reihenfolge\n",
    "import random\n",
    "random.shuffle(data_and_names)\n",
    "\n",
    "# Teile die kombinierten und zufällig angeordneten Listen in Trainings- und Testlisten auf (60% Train, 40% Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tuples, test_tuples = train_test_split(data_and_names, test_size=0.4, random_state=42)\n",
    "\n",
    "# Teile die Trainings- und Testlisten wieder in die ursprünglichen Listen auf\n",
    "train_data, train_names = zip(*train_tuples)\n",
    "train_data = np.array(train_data)\n",
    "test_data, test_names = zip(*test_tuples)\n",
    "test_data = np.array(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Führen Sie anschließend eine Hauptkomponentenanalyse auf den Trainingsdaten durch und projizieren Sie sowohl Trainings- als auch Testbilder auf die ersten 7 Eigengesichter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(train_data)\n",
    "\n",
    "from pca import pca\n",
    "num_components = len(X.columns)\n",
    "X_pca, Sigma, V = pca(X, num_components)\n",
    "\n",
    "# Umrechnung von Singulärwerten in Eigenwerte\n",
    "eigenvalues = (Sigma**2) / (len(X) - 1)\n",
    "\n",
    "# Gesamtvarianz\n",
    "total_variance = sum(eigenvalues)\n",
    "\n",
    "# Erklärte Varianz für jede Komponente\n",
    "explained_variances = [(i / total_variance) * 100 for i in eigenvalues]\n",
    "\n",
    "# Kumulative erklärte Varianz\n",
    "cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Tabellarische Darstellung\n",
    "results = pd.DataFrame({\n",
    "    'Eigenwert': eigenvalues,\n",
    "    'Erklärte Varianz (%)': explained_variances,\n",
    "    'Kumulative erklärte Varianz (%)': cumulative_variances\n",
    "})\n",
    "\n",
    "print(\"Ergebnisse der PCA-Analyse:\")\n",
    "print(results.head(150))\n",
    "\n",
    "# Trainingsdaten projeziert auf die ersten 7 Eigengesichter\n",
    "train_data_pca = X_pca[:,:7]\n",
    "\n",
    "# Testdaten zentrieren\n",
    "test_data_meaned = pd.DataFrame(test_data) - np.mean(X, axis=0)\n",
    "\n",
    "# Testdaten standardisieren\n",
    "test_data_std = np.std(test_data_meaned, axis=0)\n",
    "test_data_standardized = test_data_meaned / test_data_std\n",
    "\n",
    "# Projektion der Testdaten auf die ersten 7 Eigengesichter\n",
    "test_data_pca = np.dot(test_data_standardized, V[:,:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trainieren Sie Ihren GNB-Klassifikator auf dem Trainingsdatensatz als ''George-W.-Bush-Detektor'', d.h. alle zu dieser Person gehörigen Bilder werden mit 1 gelabelt, alle sonstigen mit –1. Werten Sie Ihren Klassifikator sowohl auf den Trainings- wie auf den unabhängigen Testdaten aus. Bestimmen Sie dafür jeweils die Detektionswahrscheinlichkeit, Richtig-Negativ-Rate, Fehlalarmrate und Falsch-Negativ-Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [1 if name == 'George_W_Bush' else -1 for name in train_names]\n",
    "\n",
    "\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
