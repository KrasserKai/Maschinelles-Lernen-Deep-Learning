{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Eigengesichter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Laden Sie sich den oben angegebenen Datensatz herunter. Erstellen Sie ein Python- Skript, dass die Verzeichnisse des Datensatzes durchsucht und die Personen ermittelt, für die mindestens 70 Bilder existieren. Die dafür geeigneten Funktionen finden sich im Standardmodul os bzw. os.path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personen mit mindestens 70 Bildern:\n",
      "Ariel_Sharon: 77 Bilder\n",
      "Colin_Powell: 236 Bilder\n",
      "Donald_Rumsfeld: 121 Bilder\n",
      "George_W_Bush: 530 Bilder\n",
      "Gerhard_Schroeder: 109 Bilder\n",
      "Hugo_Chavez: 71 Bilder\n",
      "Tony_Blair: 144 Bilder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Erstellen Sie ein Python-Skript, das alle Bilder bis auf eines pro Person (diese werden später zum Testen des Klassifikators gebraucht) dieser am häufigsten abgebildeten Personen lädt, diese in Vektoren stackt und dann in einer gemeinsamen Designmatrix ablegt. Zum Laden der Bilder in Numpy-Arrays verwenden Sie am einfachsten das Modul scikit-image. Schneiden Sie zunächst einen einheitlichen zentralen Ausschnitt aus, der nur Augen und Mund enthält. Skalieren Sie die Bilder auf die Größe 32 × 32. Achten Sie darauf, vorher die Farbbilder in Grauwerte umzuwandeln (z.B. mit der Option as_gray = True) Legen Sie zusätzlich einen Vektor an, in dem der Name der Person (d.h. der Ordnername) für jede Zeile steht. Führen Sie die gleiche Art der Verarbeitung mit dem übrig gebliebenen Testbild pro Person durch und speichern Sie diese getrennt ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "dataset_path    = extracted_path\n",
    "train_data_path = \".lfw-dataset-train\"\n",
    "test_data_path  = \".lfw-dataset-test\"\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "os.makedirs(test_data_path, exist_ok=True)\n",
    "\n",
    "# Trainingsdaten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(dataset_path, person_folder)\n",
    "    train_person_data = []\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Zuschneiden und skalieren auf 32x32\n",
    "            img = transform.resize(img[50:150, 50:150], (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "            train_person_data.append(img_vector)\n",
    "\n",
    "    # Daten in einer gemeinsamen Designmatrix speichern\n",
    "    train_person_data = np.array(train_person_data)\n",
    "    np.save(os.path.join(train_data_path, f\"{person_folder}_train.npy\"), train_person_data)\n",
    "\n",
    "# Testdaten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(dataset_path, person_folder)\n",
    "    test_person_data = []\n",
    "\n",
    "    test_image_path = os.path.join(person_path, f\"{person_folder}_0001.jpg\")\n",
    "    # Laden und in Graustufen konvertieren\n",
    "    img = io.imread(test_image_path, as_gray=True)\n",
    "    # Zuschneiden und skalieren auf 32x32\n",
    "    img = transform.resize(img[50:150, 50:150], (32, 32))\n",
    "    # In einen Vektor packen\n",
    "    img_vector = img.flatten()\n",
    "    test_person_data.append(img_vector)\n",
    "\n",
    "    # Daten getrennt abspeichern\n",
    "    test_person_data = np.array(test_person_data)\n",
    "    np.save(os.path.join(test_data_path, f\"{person_folder}_test.npy\"), test_person_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Wenden Sie nun Ihre Hauptkomponentenanalyse aus Arbeitsblatt 1 auf Ihre Designmatrix (Achtung: kopieren Sie alle Trainingsbilder für alle Personen als Zeilen in eine gemeinsame Designmatrix!) an. Stellen Sie die ersten 150 Eigenwerte in einem Diagramm und die ersten 12 Eigengesichter durch Umformung der gestackten Darstellung in das ursprüngliche Bildformat dar. Interpretieren Sie das Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse der PCA-Analyse:\n",
      "      Eigenwert  Erklärte Varianz (%)  Kumulative erklärte Varianz (%)\n",
      "0    257.423576             25.119503                        25.119503\n",
      "1    185.498152             18.100989                        43.220493\n",
      "2     84.995729              8.293920                        51.514412\n",
      "3     75.442555              7.361717                        58.876129\n",
      "4     36.440521              3.555882                        62.432011\n",
      "..          ...                   ...                              ...\n",
      "145    0.286574              0.027964                        97.204736\n",
      "146    0.283656              0.027679                        97.232416\n",
      "147    0.277384              0.027067                        97.259483\n",
      "148    0.274706              0.026806                        97.286289\n",
      "149    0.269238              0.026272                        97.312561\n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "# Durchsuchen Sie das Verzeichnis und laden Sie die Matrizen\n",
    "for filename in os.listdir(train_data_path):\n",
    "    train_person_data = np.load(os.path.join(train_data_path, filename))\n",
    "    train_data.append(train_person_data)\n",
    "\n",
    "X = pd.DataFrame(np.concatenate(train_data, axis=0))\n",
    "\n",
    "from pca import pca\n",
    "num_components = len(X.columns)\n",
    "X_pca, Sigma, V = pca(X, num_components)\n",
    "\n",
    "# Umrechnung von Singulärwerten in Eigenwerte\n",
    "eigenvalues = (Sigma**2) / (len(X) - 1)\n",
    "\n",
    "# Gesamtvarianz\n",
    "total_variance = sum(eigenvalues)\n",
    "\n",
    "# Erklärte Varianz für jede Komponente\n",
    "explained_variances = [(i / total_variance) * 100 for i in eigenvalues]\n",
    "\n",
    "# Kumulative erklärte Varianz\n",
    "cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Tabellarische Darstellung\n",
    "results = pd.DataFrame({\n",
    "    'Eigenwert': eigenvalues,\n",
    "    'Erklärte Varianz (%)': explained_variances,\n",
    "    'Kumulative erklärte Varianz (%)': cumulative_variances\n",
    "})\n",
    "\n",
    "print(\"Ergebnisse der PCA-Analyse:\")\n",
    "print(results.head(150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo Umformung der ersten 12 Eigengesichter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Von den Testbildern wird nun ebenfalls der Mittelwert der Trainingsdaten abgezogen (s. Schritt 1 im PCA-Algorithmus). Projizieren Sie jedes der Trainings- und Testbilder auf die ersten 7 Eigengesichter, d.h. Sie erhalten so für jedes Trainings- und Testbild 7 Merkmale. Die Gesichtserkennung geschieht nun dadurch, dass Sie den euklidischen Abstand des Testbildes in diesem 7-dimensionalen Merkmalsraum zu allen Trainingsbildern berechnen. Die Person des am nächsten liegenden Trainingsbildes (d.h. mit dem minimalen euklidischen Abstand) ist dann (vermutlich) auch die korrekte Person für das Testbild (Nächster-Nachbar-Klassifikator). Welche Bilder werden korrekt klassifiziert, welche Verwechslungen gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
