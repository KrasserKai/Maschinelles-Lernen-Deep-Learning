{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Eigengesichter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Laden Sie sich den oben angegebenen Datensatz herunter. Erstellen Sie ein Python- Skript, dass die Verzeichnisse des Datensatzes durchsucht und die Personen ermittelt, für die mindestens 70 Bilder existieren. Die dafür geeigneten Funktionen finden sich im Standardmodul os bzw. os.path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "lfw_filename = 'lfw-funneled.tgz'\n",
    "lfw_directory = '.lfw-dataset'\n",
    "\n",
    "if not os.path.isfile(lfw_filename):\n",
    "    print(\"Downloading\")\n",
    "    urlretrieve('http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz',filename = lfw_filename)\n",
    "\n",
    "if not os.path.isdir(lfw_directory):\n",
    "    # Dateien in das Zielverzeichnis extrahieren\n",
    "    with tarfile.open(lfw_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=lfw_directory)\n",
    "\n",
    "min_images_required = 70\n",
    "selected_persons = []\n",
    "extracted_path = \".lfw-dataset/lfw_funneled\"\n",
    "\n",
    "# Verzeichnis durchsuchen\n",
    "for person_folder in os.listdir(extracted_path):\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "\n",
    "    if os.path.isdir(person_path):\n",
    "        # Anzahl der Bilder fuer die aktuelle Person zaehlen\n",
    "        num_images = len([f for f in os.listdir(person_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Ueberpruefen, ob Mindestanzahl erreicht\n",
    "        if num_images >= min_images_required:\n",
    "            selected_persons.append({\n",
    "                'person_name': person_folder,\n",
    "                'num_images': num_images\n",
    "            })\n",
    "\n",
    "print(f\"Personen mit mindestens {min_images_required} Bildern:\")\n",
    "for person_info in selected_persons:\n",
    "    print(f\"{person_info['person_name']}: {person_info['num_images']} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Erstellen Sie ein Python-Skript, das alle Bilder bis auf eines pro Person (diese werden später zum Testen des Klassifikators gebraucht) dieser am häufigsten abgebildeten Personen lädt, diese in Vektoren stackt und dann in einer gemeinsamen Designmatrix ablegt. Zum Laden der Bilder in Numpy-Arrays verwenden Sie am einfachsten das Modul scikit-image. Schneiden Sie zunächst einen einheitlichen zentralen Ausschnitt aus, der nur Augen und Mund enthält. Skalieren Sie die Bilder auf die Größe 32 × 32. Achten Sie darauf, vorher die Farbbilder in Grauwerte umzuwandeln (z.B. mit der Option as_gray = True) Legen Sie zusätzlich einen Vektor an, in dem der Name der Person (d.h. der Ordnername) für jede Zeile steht. Führen Sie die gleiche Art der Verarbeitung mit dem übrig gebliebenen Testbild pro Person durch und speichern Sie diese getrennt ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "train_data = []\n",
    "train_data_name = []\n",
    "test_data = []\n",
    "test_data_name = []\n",
    "\n",
    "# Daten verarbeiten\n",
    "for person in selected_persons:\n",
    "    person_folder = person['person_name']\n",
    "    person_path = os.path.join(extracted_path, person_folder)\n",
    "    train_person_data = []\n",
    "    test_person_data = []\n",
    "    first = True\n",
    "\n",
    "    # Bilder laden\n",
    "    for filename in os.listdir(person_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(person_path, filename)\n",
    "            # Laden und in Graustufen konvertieren\n",
    "            img = io.imread(image_path, as_gray=True)\n",
    "            # Skalieren auf 32x32\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            # In einen Vektor packen\n",
    "            img_vector = img.flatten()\n",
    "\n",
    "            if (first):\n",
    "                test_person_data.append(img_vector)\n",
    "                test_data_name.append(person_folder)\n",
    "                first = False\n",
    "            else:\n",
    "                train_person_data.append(img_vector)\n",
    "                train_data_name.append(person_folder)\n",
    "\n",
    "    # Trainingsdaten in einer gemeinsamen Designmatrix speichern\n",
    "    train_person_data = np.array(train_person_data)\n",
    "    train_data.append(train_person_data)\n",
    "\n",
    "    # Testdaten getrennt abspeichern\n",
    "    test_person_data = np.array(test_person_data)\n",
    "    test_data.append(test_person_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Wenden Sie nun Ihre Hauptkomponentenanalyse aus Arbeitsblatt 1 auf Ihre Designmatrix (Achtung: kopieren Sie alle Trainingsbilder für alle Personen als Zeilen in eine gemeinsame Designmatrix!) an. Stellen Sie die ersten 150 Eigenwerte in einem Diagramm und die ersten 12 Eigengesichter durch Umformung der gestackten Darstellung in das ursprüngliche Bildformat dar. Interpretieren Sie das Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.concatenate(train_data, axis=0))\n",
    "\n",
    "from pca import pca\n",
    "num_components = len(X.columns)\n",
    "X_pca, Sigma, V = pca(X, num_components)\n",
    "\n",
    "# Umrechnung von Singulärwerten in Eigenwerte\n",
    "eigenvalues = (Sigma**2) / (len(X) - 1)\n",
    "\n",
    "# Gesamtvarianz\n",
    "total_variance = sum(eigenvalues)\n",
    "\n",
    "# Erklärte Varianz für jede Komponente\n",
    "explained_variances = [(i / total_variance) * 100 for i in eigenvalues]\n",
    "\n",
    "# Kumulative erklärte Varianz\n",
    "cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Tabellarische Darstellung\n",
    "results = pd.DataFrame({\n",
    "    'Eigenwert': eigenvalues,\n",
    "    'Erklärte Varianz (%)': explained_variances,\n",
    "    'Kumulative erklärte Varianz (%)': cumulative_variances\n",
    "})\n",
    "\n",
    "print(\"Ergebnisse der PCA-Analyse:\")\n",
    "print(results.head(150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors = V[:, :12]\n",
    "\n",
    "eigenvalue_images = []\n",
    "for i in range(12):\n",
    "    eigenvalue_image = eigenvectors[:, i].reshape(32, 32)\n",
    "    eigenvalue_images.append(eigenvalue_image)\n",
    "\n",
    "# Visualisieren\n",
    "for i in range(12):\n",
    "    plt.imshow(eigenvalue_images[i], cmap='gray')\n",
    "    plt.title(f'Eigengesicht {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Von den Testbildern wird nun ebenfalls der Mittelwert der Trainingsdaten abgezogen (s. Schritt 1 im PCA-Algorithmus). Projizieren Sie jedes der Trainings- und Testbilder auf die ersten 7 Eigengesichter, d.h. Sie erhalten so für jedes Trainings- und Testbild 7 Merkmale. Die Gesichtserkennung geschieht nun dadurch, dass Sie den euklidischen Abstand des Testbildes in diesem 7-dimensionalen Merkmalsraum zu allen Trainingsbildern berechnen. Die Person des am nächsten liegenden Trainingsbildes (d.h. mit dem minimalen euklidischen Abstand) ist dann (vermutlich) auch die korrekte Person für das Testbild (Nächster-Nachbar-Klassifikator). Welche Bilder werden korrekt klassifiziert, welche Verwechslungen gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdaten zentrieren\n",
    "test_data_meaned = pd.DataFrame(np.concatenate(test_data, axis=0)) - np.mean(X, axis=0)\n",
    "# Testdaten standardisieren\n",
    "test_data_std = np.std(test_data_meaned, axis=0)\n",
    "test_data_standardized = test_data_meaned / test_data_std\n",
    "# Projektion der Testdaten auf die ersten 7 Eigengesichter\n",
    "test_data_pca = np.dot(test_data_standardized, V[:,:7])\n",
    "\n",
    "# Trainingsdaten projeziert auf die ersten 7 Eigengesichter\n",
    "train_data_pca = X_pca[:,:7]\n",
    "\n",
    "# Differenzen zwischen den Test- und Trainingsbildern berechnen\n",
    "differences = train_data_pca - test_data_pca[:, np.newaxis, :]\n",
    "\n",
    "i = 0\n",
    "for test_picture in test_data_pca:\n",
    "    # Differenzen zwischen den Test- und Trainingsbildern berechnen\n",
    "    differences = train_data_pca - test_picture\n",
    "    # Euklidischen Abstand für jeden Vektor berechnen\n",
    "    euclidean_distances = np.linalg.norm(differences, axis=1)\n",
    "    \n",
    "    # Bilder mit kleinstem Abstand finden\n",
    "    min_index = np.argmin(euclidean_distances, axis=0)\n",
    "    print(f\"Soll: {test_data_name[i]}  Ist: {train_data_name[min_index]} \")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
