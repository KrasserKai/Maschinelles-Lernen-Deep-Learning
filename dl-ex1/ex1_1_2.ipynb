{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spielzeugdatensatz\n",
    "Laden Sie sich das Jupyter-Notebook ”Training eines MLPs auf MNIST” von der Vorlesungsseite\n",
    "auf Moodle herunter, das den Numpy-Code für den Backpropagation-Algorithmus für\n",
    "MLPs enthält. Statt auf MNIST werden wir zunächst ein einfacheres Perzeptron auf den\n",
    "Spielzeugdaten aus der Vorlesung trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import load_mnist\n",
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Erzeugen Sie 200 zweidimensionale Trainingsdatenpunkte mithilfe einer Gleichverteilung\n",
    "über dem Gebiet [−6, 6]×[−6, 6]. Speichern Sie diese in einer 200×2 Designmatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.uniform(-6,6,200)\n",
    "X2 = np.random.uniform(-6,6,200)\n",
    "X = np.column_stack((X1,X2))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Die Klassenlabels 0 und 1 werden so vergeben, dass alle Datenpunkte im 1. und 3.\n",
    "Quadranten das Label 1 und im 2. und 4. Quadranten das Label 0 erhalten. Speichern\n",
    "Sie die Labels in einem Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X[:,0]<0\n",
    "x2 = X[:,1]>0\n",
    "x_q1 = x1*x2\n",
    "y_q1 = x_q1*1\n",
    "\n",
    "x3 = X[:,0]>0\n",
    "x4 = X[:,1]<0\n",
    "x_q3 = x3*x4\n",
    "y_q3 = x_q3*1\n",
    "\n",
    "y_ges = y_q1 + y_q3 \n",
    "y_ges = np.array([y_ges]).T\n",
    "\n",
    "X_train = X\n",
    "y_train = y_ges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Erzeugen Sie einen gleich großen Testdatensatz nach demselben Prinzip. Stellen Sie\n",
    "beide Datensätze zur Überprüfung als Scatterplot dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testmatrix erzeugen\n",
    "X1 = np.random.uniform(-6,6,200)\n",
    "X2 = np.random.uniform(-6,6,200)\n",
    "X = np.column_stack((X1,X2))\n",
    "\n",
    "# Klassenlabels entsprechend Quadrant vergeben\n",
    "x1 = X[:,0]<0\n",
    "x2 = X[:,1]>0\n",
    "x_q1 = x1*x2\n",
    "y_q1 = x_q1*1\n",
    "\n",
    "x3 = X[:,0]>0\n",
    "x4 = X[:,1]<0\n",
    "x_q3 = x3*x4\n",
    "y_q3 = x_q3*1 \n",
    "\n",
    "y_ges = y_q1 + y_q3 \n",
    "y_ges = np.array([y_ges]).T\n",
    "\n",
    "X_test = X\n",
    "y_test = y_ges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Trainingsdaten\")\n",
    "plt.scatter(X_train[:,0], X_train[:,1],c=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Testdaten\")\n",
    "plt.scatter(X_test[:,0], X_test[:,1],c=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Das in der Vorlesung dargestellte Experiment operiert nicht direkt auf den Inputdaten,\n",
    "sondern auf 2 Merkmalen, die mithilfe zweier Neuronen mit fixem Gewichtsvektor\n",
    "berechnet werden: ein Neuron teilt die Inputebene waagrecht entlang der x-Achse, das\n",
    "andere senkrecht entlang der y-Achse. Wie muss der Gewichtsvektor für das jeweilige\n",
    "Neuron aussehen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 =np.array ([(1,0)])\n",
    "w2 = np.array ([(0,1)])\n",
    "w1*X_train #neuron 1\n",
    "w2*X_train #neuron 2\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Berechnen Sie die Entscheidungsfunktion beider Neuron mit der im Beispielcode\n",
    "angegebenen sigmoiden Aktivierungsfunktion auf einem 100 × 100-Gitter innerhalb\n",
    "des Gebietes [−6, 6] × [−6, 6] und stellen Sie diese zur Überprüfung als Farbbild dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid (vektorisiert)\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# Ableitung des Sigmoids\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# Ableitung der MSE-Kostenfunktion\n",
    "def cost_derivative(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations.\"\"\"\n",
    "    return (output_activations-y)\n",
    "\n",
    "# Funktion zur Berechnung der Entscheidungsfunktion eines Neurons\n",
    "def Entscheidungsfunktion(x, y, axis):\n",
    "    if axis == 'x':\n",
    "        return sigmoid(x)\n",
    "    elif axis == 'y':\n",
    "        return sigmoid(y)\n",
    "\n",
    "# Gitter erstellen\n",
    "x_vals = np.linspace(-6, 6, 100)\n",
    "y_vals = np.linspace(-6, 6, 100)\n",
    "x_grid, y_grid = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# Neuron 1: teilt die Inputebene waagrecht entlang der x-Achse\n",
    "neuron1 = Entscheidungsfunktion(x_grid, y_grid, 'x')\n",
    "\n",
    "# Neuron 2: teilt die Inputebene senkrecht entlang der y-Achse\n",
    "neuron2 = Entscheidungsfunktion(x_grid, y_grid, 'y')\n",
    "\n",
    "# Darstellung der Ergebnisse als Farbbilder\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(x_grid, y_grid, neuron1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(x_grid, y_grid, neuron2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Da die Eingangsneuronen nicht mittrainiert werden, können wir deren Output schon\n",
    "im Vorfeld berechnen. Erzeugen Sie dazu neue Designmatritzen für den Trainings und\n",
    "Testdatensatz, so dass die erste Spalte den Output des ersten Neurons und die\n",
    "zweite Spalte den Output des zweiten Neurons enthält. Erzeugen Sie auch eine entsprechende\n",
    "Designmatrix für Ihr 100 × 100-Gitter, das wir später zu Darstellungszwecken\n",
    "brauchen werden. Wichtig: arbeiten Sie im Folgenden nur mit diesen transformierten\n",
    "Designmatritzen, um korrekte Ergebnisse zu erhalten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gitter für Trainingsdaten erstellen\n",
    "x_train_vals = np.linspace(-6, 6, 100)\n",
    "y_train_vals = np.linspace(-6, 6, 100)\n",
    "\n",
    "# Designmatrix für Trainingsdaten erstellen\n",
    "train_design_matrix = np.array([x_train_vals, y_train_vals]).T\n",
    "\n",
    "# Gitter für Testdaten erstellen (nehmen wir an, es ist ein anderes Gitter als das für das 100x100-Gitter)\n",
    "x_test_vals = np.linspace(-6, 6, 50)\n",
    "y_test_vals = np.linspace(-6, 6, 50)\n",
    "\n",
    "# Designmatrix für Testdaten erstellen\n",
    "test_design_matrix = np.array([x_test_vals, y_test_vals]).T\n",
    "\n",
    "# Gitter für das 100x100-Gitter erstellen\n",
    "x_grid_vals = np.linspace(-6, 6, 100)\n",
    "y_grid_vals = np.linspace(-6, 6, 100)\n",
    "\n",
    "# Designmatrix für das 100x100-Gitter erstellen\n",
    "grid_design_matrix = np.array([x_grid_vals, y_grid_vals]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training eines MLPs auf den Spielzeugdaten\n",
    "Der Code aus dem Beispielnotebook muss zunächst an das Szenario aus der Vorlesung angepasst\n",
    "werden: ein deutlich kleineres Netzwerk mit eindimensionalem statt zehndimensionalem\n",
    "Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setzen Sie im Code die Größe der Minibatches auf 10, die Anzahl der Epochen\n",
    "auf 150 und die Lernrate auf 0.03. Ändern Sie die Netzarchitektur so ab, dass sie 2\n",
    "Eingangsneuronen, 2 verdeckte Schichten mit jeweils 2 Neuronen und 1 Ausgangsneuron\n",
    "haben. Überprüfen Sie die Größen der sich daraus ergebenden Gewichtsmatritzen auf\n",
    "Korrektheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netzwerkparameter\n",
    "mbs = 10                    # Größe der Minibatches\n",
    "eta = 0.03                  # Lernrate\n",
    "no_hidden = 2               # Anzahl versteckter Neuronen\n",
    "epochs = 150                # Anzahl Epochen\n",
    "sizes = [2, no_hidden, 1]   # dreischichtiges MPL mit 2 Eingangs-, no_hidden versteckten, 1 Ausgangsneuronen\n",
    "num_layers = 2              # Anzahl Schichten\n",
    "\n",
    "# Arrays für Gewichte und Schwellwerte (initialisiert mit Gaußschem Rauschen)\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] # Schwellwerte\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] #Gewichte\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Der Beispielcode verwendet One-Hot-Coding für die Labels, in unserem Beispiel sind\n",
    "die Klassenzugehörigkeiten aber durch die Klassenindizes 0 und 1 codiert. Wir müssen\n",
    "daher die Funktion evaluate() im Code so abändern, dass ein Beispiel als korrekt\n",
    "klassifiziert gilt, wenn bei Klasse 0 der MLP-Output kleiner als 0.5 ist und bei Klasse 1\n",
    "größer als 0.5. Berechnen Sie zusätzlich den MSE in dieser Funktion bei jedem Aufruf\n",
    "uns speichern Sie diesen in einem zusätzlichen Array ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(a):\n",
    "    \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a\n",
    "\n",
    "def backprop(x, y):\n",
    "    \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "    gradient for the cost function C_x.  ``nabla_b`` and\n",
    "    ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    \n",
    "    # Initialisiere Updates für Schwellwerte und Gewichte\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Vorwärtslauf\n",
    "    activation = x # Initialisierung a^1 = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Rückwärtslauf\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # Fehler am Output\n",
    "    nabla_b[-1] = delta # Update Schwellwert in der Ausgangsschicht\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Update Gewichte in der Ausgangsschicht\n",
    "    for l in range(2, num_layers): # Backpropagation\n",
    "        z = zs[-l] # gewichteter Input\n",
    "        sp = sigmoid_prime(z) # Ableitung der Aktivierungsfunktion\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sp # Fehler in Schicht l\n",
    "        nabla_b[-l] = delta # Update Schwellwert \n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Update Gewichte\n",
    "\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "def update_mini_batch(xmb, ymb, eta):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation to a single mini batch.\n",
    "    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "    is the learning rate.\"\"\"\n",
    "    global weights\n",
    "    global biases\n",
    "\n",
    "    # Initialisiere Updates für Schwellwerte und Gewichte\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Gehe durch alle Beispielpaare im Minibatch\n",
    "    for i in range(xmb.shape[0]):\n",
    "        x = np.reshape(xmb[i,:],(xmb.shape[1],1)).copy()\n",
    "        if len(ymb.shape) == 2:\n",
    "            y = np.reshape(ymb[i,:],(ymb.shape[1],1)).copy()\n",
    "        else:\n",
    "            y = ymb[i].copy()\n",
    "        \n",
    "        # Berechne Updates für alle Schichten über Backprop\n",
    "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "        \n",
    "        # Addiere einzelne Updates auf\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    \n",
    "    # Berechne neue Gewichte\n",
    "    weights = [w-(eta/xmb.shape[0])*nw\n",
    "                    for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/xmb.shape[0])*nb\n",
    "                   for b, nb in zip(biases, nabla_b)]\n",
    "    \n",
    "    return (weights, biases)\n",
    "\n",
    "def evaluate(x2, y2):\n",
    "    \"\"\"Return the number of test inputs for which the neural\n",
    "    network outputs the correct result. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    \n",
    "    correct = 0 # Anzahl korrekt klassifizierter Testbeispiele\n",
    "    diffsum = 0 # Hilfesvariable zum berechnen des MSE\n",
    "    \n",
    "    # Gehe den Testdatensatz durch\n",
    "    for i in range(0, x2.shape[0]):\n",
    "        x = np.reshape(x2[i,:],(x2.shape[1],1)).copy()\n",
    "        if len(y2.shape) == 2:\n",
    "            y = np.reshape(y2[i,:],(y2.shape[1],1)).copy()\n",
    "        else:\n",
    "            y = y2[i].copy()\n",
    "        \n",
    "        # Vorwärtslauf\n",
    "        ypred = feedforward(x)\n",
    "        \n",
    "        # Übernehme Klasse von Label\n",
    "        c = y\n",
    "\n",
    "        print(c)\n",
    "        print(ypred)\n",
    "\n",
    "        # Index des maximal aktivierten Outputs ist die Entscheidung des Netzwerk\n",
    "        if (ypred < 0,5):\n",
    "            cpred = 0\n",
    "        else:\n",
    "            cpred = 1\n",
    "        \n",
    "        # Falls beide übereinstimmen, addiere zur Gesamtzahl\n",
    "        if c == cpred:\n",
    "            correct += 1\n",
    "\n",
    "        diff = (c-cpred)**2\n",
    "        diffsum += diff\n",
    "    # MSE\n",
    "    MSE = (1/len(y2.shape))*(diffsum)\n",
    "        \n",
    "    return correct\n",
    "\n",
    "def SGD(x0, y0, epochs, mini_batch_size, eta, x2, y2):\n",
    "\n",
    "    n_test = x2.shape[0] # Anzahl Testdaten\n",
    "    n = x0.shape[0]      # Anzahl Trainingsdaten\n",
    "    \n",
    "    # gehe durch alle Epochen\n",
    "    acc_val = np.zeros(epochs)\n",
    "    for j in range(epochs):\n",
    "        \n",
    "        # Bringe die Trainingsdaten in eine zufällige Reihenfolge für jede Epoche\n",
    "        p = np.random.permutation(n) # Zufällige Permutation aller Indizes von 0 .. n-1\n",
    "        x0 = x0[p,:]\n",
    "        y0 = y0[p]\n",
    "        \n",
    "        # Zerlege den permutierten Datensatz in Minibatches \n",
    "        for k in range(0, n, mini_batch_size):\n",
    "            xmb = x0[k:k+mini_batch_size,:]\n",
    "            if len(y0.shape) == 2:\n",
    "                ymb = y0[k:k+mini_batch_size,:]\n",
    "            else:\n",
    "                ymb = y0[k:k+mini_batch_size]\n",
    "            update_mini_batch(xmb, ymb, eta)\n",
    "        \n",
    "        # Gib Performance aus\n",
    "        acc_val[j] = evaluate(x2, y2)\n",
    "        print(\"Epoch {0}: {1} / {2}\".format(j, acc_val[j], n_test))\n",
    "    \n",
    "    return acc_val\n",
    "\n",
    "acc_val = SGD(train_design_matrix, y_train_vals, epochs, mbs, eta, test_design_matrix, y_test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%version_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
